__version__ = "0.0.1"
# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/__init__.ipynb.

# %% auto 0
__all__ = ['ReconstructionResults', 'ReconstructionOptions', 'ReconstructionInputs', 'reconstruct',
            'preprocess',
           'build_solver',
           'SolverOptions']

# %% ../nbs/__init__.ipynb 1
from .core import HDF5Dataset as HDF5Dataset
from dataclasses import dataclass
from typing import List, Union
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from .operators.loss import amplitude_loss
import torch as th
from scatterem.util.base import (orthogonalize_probe, shrink_nonneg, apply_butterworth_filter, create_3d_gaussian_kernel,
                                 init_gauss_blur_regularization, gaussian_blur_regularization)
import numpy as np
from torch.utils.data import BatchSampler, SequentialSampler, RandomSampler
from .util.plot import zplot
from scatterem.models import MultiSlicePtychographyModel

@dataclass
class ReconstructionResults:
    V: th.Tensor
    probes: List
    positions: List
    losses: th.Tensor


@dataclass
class ReconstructionOptions:
    optimize_probe = lambda it: it > -1
    optimize_sample = lambda it: it > -1
    num_iterations: int = 500
    bin_factor: int = 1
    propagator = lambda it: it
    probe_optimizer = optim.SGD
    object_optimizer = optim.SGD
    probe_optimizer_parameters = {'lr': 30e-3, 'momentum': 0.9}
    object_optimizer_parameters = {'lr': 30e-3, 'momentum': 0.5}
    after_iteration_hooks = []
    after_dataset_hooks = []
    after_batch_hooks = []
    device = None
    batch_size = 1


import torch as th
import skimage

from .models import BaseModel


@dataclass
class ReconstructionInputs:
    model: BaseModel
    # model: MultiSlicePtychographyModel
    datasets: List[Dataset]


def reconstruct(inputs: ReconstructionInputs, reconstruction_options: ReconstructionOptions):
    opt = reconstruction_options

    model = inputs.model
    datasets = inputs.datasets
    error_norm_total = th.sum(th.stack(model.error_norm))

    object_optimizer = opt.object_optimizer
    batch_size = opt.batch_size

    sample_optimizer = object_optimizer([model.object], **opt.object_optimizer_parameters)

    q_lowpass = (model.object.shape[1]/2 - model.braggpix)/(model.dx * model.object.shape[1]/2)
    sampling = model.dx
    butterworth_order = model.butterorder


    losses = []
    for i in range(opt.num_iterations):
        losses_it = []
        j = 0
        for dataset in datasets:
            # print(f'dataset {j:02d}')
            sampler = BatchSampler(RandomSampler(range(len(dataset))), batch_size=batch_size, drop_last=False)
            pin_memory = str(dataset.data.device) == 'cpu'
            dataloader = DataLoader(dataset, sampler=sampler, batch_sampler=None, batch_size=None,
                                    pin_memory=pin_memory, drop_last=False)

            for batch_index, probe_index, angles_index, r_indices, translation_index, start_end_index, amplitudes_target in dataloader:
                r = model.positions[angles_index][r_indices]
                dr = model.dr[angles_index][r_indices]
                probe_model = model.probe[angles_index][probe_index]
                angle = model.angles[:, angles_index]
                translations = model.translations[translation_index]
                start_end = model.start_end_model[start_end_index]
                amplitudes_target = amplitudes_target.to(r.device)

                probe_optimizer = opt.probe_optimizer([probe_model], **opt.probe_optimizer_parameters)
                # probe_optimizer = probe_optimizer_list[angles_index][probe_index]

                if opt.optimize_probe(i):
                    probe_model.requires_grad = True
                    probe_optimizer.zero_grad()

                if opt.optimize_sample(i):
                    model.object.requires_grad = True
                    sample_optimizer.zero_grad()

                object_patches = model.object_model(model.object,
                                                    probe_model,
                                                    r,
                                                    angle,
                                                    translations,
                                                    model.bin_factor,
                                                    start_end)

                amplitudes_model = model.measurement_model(object_patches, r, dr, probe_model, model.propagator,
                                                           model.intensity_scaling_factor)



                loss, loss_per_pattern = model.loss_function(amplitudes_model, amplitudes_target)
                losses_it.append(loss.item())

                loss.backward()

                if reconstruction_options.optimize_sample(i):
                    model.scale_object_gradient(model.object)
                    sample_optimizer.step()

                    if i > 250:
                        with th.no_grad():
                            model.object = shrink_nonneg(model.object, model.tau1, model.tau2)

                if reconstruction_options.optimize_probe(i):
                    model.scale_probe_gradient(probe_model)
                    probe_optimizer.step()
                    probe_model = orthogonalize_probe(probe_model, probe_model.device)
                    probe_model.requires_grad = True
                    model.probe[angles_index][probe_index] = probe_model
                    # model.probe[0][0] = probe_model

                for hook in opt.after_batch_hooks:
                    hook(i, model.object, model.probe, model.positions)

            error_norm = model.error_norm[j]
            # error_norm = model.error_norm[0]
            loss_norm = losses_it[-1] / error_norm
            j += 1
            for hook in opt.after_dataset_hooks:
                hook(i, model.object, model.probe, model.positions)

            if j % 1 == 0:
                # print(f'i {i} loss {losses_ang[-1]:f}')
                print(f'angle {j}  {loss_norm:f}')
                # print(f'angle {j} volume grad: {th.sum(model.object.grad).real}')

        # print(f'angle {np.rad2deg(angle[0].cpu().numpy()):2.2g}')
        losses_it = np.array(losses_it).sum()
        # sample_scheduler.step(losses_it)
        losses.append(losses_it)

        if i > 200:
            model.object.requires_grad_(False)
            model.object = apply_butterworth_filter(model.object.cpu(), q_lowpass, butterworth_order, sampling)
            # model.object = filter3d(model.object.unsqueeze(0).cpu(), tukey).squeeze(0)
            model.object = model.object.to(probe_model.device).type(th.complex64)

        for hook in opt.after_iteration_hooks:
            hook(i, model.object, model.probe, model.positions)
        losses_it_norm = losses_it.item() / error_norm_total
        if i % 1 == 0:
            # print(f'i {i} loss {losses_it.item():2.4g}')
            print(f'i {i} loss {losses_it_norm:2.4g}')

    results = ReconstructionResults(
        positions=[[r.detach().cpu().numpy() for r in p0] for p0 in inputs.model.positions],
        V=model.object.detach().cpu().numpy(),
        probes=[[p.detach().cpu().numpy() for p in p0] for p0 in inputs.model.probe],
        losses=th.tensor(losses)
    )

    return results

def preprocess(data, metadata):
    return None


def build_solver(preproc_results, solver_options):
    res = None

    def f(data, metadata):
        return 0

    return f


class SolverOptions():
    def __int__(self):
        pass
