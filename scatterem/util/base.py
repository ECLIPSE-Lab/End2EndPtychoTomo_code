# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/util.ipynb.

# %% auto 0
__all__ = ['PARAM_PREFIX', 'add_modes_cartesian_hermite', 'regularize_multilayers', 'apply_butterworth_filter',
           'shrink_nonneg', 'create_3d_gaussian_kernel', 'init_gauss_blur_regularization',
           'gaussian_blur_regularization', 'gram_schmidt_orthogonalization', 'orthogonalize_probe', 'return_scaled_histogram_ordering',
           'get_bright_field_size', 'array_split_divpoints_ntotal', 'fftshift_checkerboard',
           'cartesian_aberrations', 'memory_mb', 'memory_gb', 'fourier_coordinates_2D', 'get_qx_qy_1D',
           'array_split_divpoints', 'distance', 'rel_dist', 'Param', 'validate_standard_param', 'format_standard_param',
           'asParam', 'make_default', 'sector_mask', 'wavelength', 'DOF', 'dense_to_sparse_kernel',
           'advanced_raster_scan', 'scatter_add_patches', 'gather_patches', 'ZernikeProbe2', 'mosaic', 'plotmosaic',
           'aperture', 'get_qx_qy_2D', 'zplot', 'plotAbsAngle', 'relativistic_mass_correction', 'energy_to_mass',
           'energy_to_wavelength', 'energy_to_sigma', 'make_probe', 'make_probe2']

# %% ../../nbs/util.ipynb 2
from numpy.fft import fftfreq
import numpy as np
import math as m
import cmath as cm
from scipy.ndimage import gaussian_filter
from kornia.filters import filter3d

# %% ../../nbs/util.ipynb 3
def add_modes_cartesian_hermite(initial_probe, num_modes: int):
    """Create more probes from a 2D Cartesian Hermite basis functions.

    Starting with the given probe, new modes are computed by multiplying it
    with a set of 2D Cartesian Hermite functions. The probes are then
    orthonormalized.

    Parameters
    ----------
    initial_probe : (..., 1, WIDTH, HEIGHT)
        A single probe basis.
    num_modes : int > 0
        The number of desired probes.

    Returns
    -------
    initial_probe : (..., num_modes, WIDTH, HEIGHT)
        New probes basis.

    References
    ----------
    Michal Odstrcil, Andreas Menzel, and Manuel Guizar-Sicaros. Iterative
    least-squares solver for generalized maximum-likelihood ptychography.
    Optics Express. 2018.
    """
    if num_modes < 1:
        raise ValueError(f"num_modes cannot be less than 1. It was {num_modes}.")
    if initial_probe.ndim < 3:
        raise ValueError(f"initial_probe is incorrect shape is should be "
                         " (..., 1, W, new_basis_functions) not {initial_probe.shape}.")

    cols = int(np.ceil(np.sqrt(num_modes)))
    rows = int(np.ceil(num_modes / cols))

    grid_x, grid_y = np.meshgrid(
        np.arange(initial_probe.shape[-2]) - (initial_probe.shape[-2] // 2 - 1),
        np.arange(initial_probe.shape[-1]) - (initial_probe.shape[-2] // 2 - 1),
        indexing='xy',
    )

    center_x = np.sum(
        grid_x * np.abs(initial_probe)**2,
        axis=(-2, -1),
        keepdims=True,
    ) / np.sum(
        np.abs(initial_probe)**2,
        axis=(-2, -1),
        keepdims=True,
    )
    center_y = np.sum(
        grid_y * np.abs(initial_probe)**2,
        axis=(-2, -1),
        keepdims=True,
    ) / np.sum(
        np.abs(initial_probe)**2,
        axis=(-2, -1),
        keepdims=True,
    )

    variance_x = np.sum(
        (grid_x - center_x)**2 * np.abs(initial_probe)**2,
        axis=(-2, -1),
        keepdims=True,
    ) / np.sum(
        np.abs(initial_probe)**2,
        axis=(-2, -1),
        keepdims=True,
    )
    variance_y = np.sum(
        (grid_y - center_y)**2 * np.abs(initial_probe)**2,
        axis=(-2, -1),
        keepdims=True,
    ) / np.sum(
        np.abs(initial_probe)**2,
        axis=(-2, -1),
        keepdims=True,
    )

    # Create basis
    basis_functions = list()
    for row_idx in range(rows):
        for col_idx in range(cols):

            current_basis = ((grid_x - center_x)**col_idx) * ((grid_y - center_y)**row_idx) * initial_probe

            if not (col_idx == 0 and row_idx == 0):
                current_basis *= np.exp(
                    -((grid_x - center_x)**2 / (2 * variance_x))
                    -((grid_y - center_y)**2 / (2 * variance_y))
                )  # yapf: disable

            current_basis /= np.linalg.norm(current_basis, axis=(-2, -1), keepdims=True)

            for existing_basis in basis_functions:
                current_basis -= existing_basis * np.inner(
                    existing_basis,
                    current_basis,
                    axis=(-2, -1),
                    keepdims=True,
                )

            current_basis /= np.linalg.norm(current_basis, axis=(-2, -1), keepdims=True)

            basis_functions.append(current_basis)

            if len(basis_functions) == num_modes:
                return np.concatenate(basis_functions, axis=-4)[:num_modes, :, :, :].astype(np.complex64)

    raise RuntimeError(
        "`add_modes_cartesian_hermite` never reached a return statement."
        " This should never happen.")


def regularize_multilayers(kernel_shape, regu_layers, alpha):
    # vol = model.detach().cpu().numpy()
    # Npix = [vol.shape[0], vol.shape[2], vol.shape[1]]
    Npix = [kernel_shape[0], kernel_shape[2], kernel_shape[1]]
    grid = []
    for i in range(len(Npix)):
        grid_ind = np.fft.ifftshift(np.arange(-np.fix(Npix[i] / 2), np.ceil(Npix[i] / 2))).T / Npix[i]
        grid_ind = np.roll(grid_ind, i)
        grid.append(grid_ind)
    X, Y, Z = np.meshgrid(grid[0], grid[1], grid[2])
    W = 1 - np.arctan((regu_layers * np.abs(Z) / np.sqrt(X ** 2 + Y ** 2 + 1e-3)) ** 2) / (np.pi / 2)
    relax = 1
    Wa = W * np.exp(-alpha * (X ** 2 + Y ** 2))
    # vol_abs_upd = np.abs(np.fft.ifftn(np.fft.fftn(vol)*np.fft.fftshift(Wa)))
    # vol_upd = th.as_tensor(vol_abs_upd, device=device).unsqueeze(0).type(th.complex64)
    return Wa

def apply_butterworth_filter(volume, freq_cutoff, filter_steepness, pixel_spacing):
    """
    Apply a Butterworth filter to a 3D volume in Fourier space.
    Used for frequency-domain filtering of reconstruction volumes.

    Parameters
    --------
    volume: np.ndarray
        Input 3D volume to be filtered
    freq_cutoff: float  
        Frequency threshold in inverse Angstroms where filter response is 0.5
    filter_steepness: float
        Controls slope of frequency rolloff. Lower values give gentler transitions
    pixel_spacing: float
        Real-space sampling interval in Angstroms per pixel

    Returns
    --------
    filtered_volume: np.ndarray
        Filtered 3D volume after applying Butterworth filter
    """
    # Get frequency coordinates
    freq_x = th.fft.fftfreq(volume.shape[1], pixel_spacing)
    freq_z = th.fft.fftfreq(volume.shape[2], pixel_spacing) 
    freq_y = th.fft.fftfreq(volume.shape[3], pixel_spacing)

    # Create 3D frequency grid
    fz, fy, fx = th.meshgrid(freq_z, freq_y, freq_x)
    freq_magnitude = th.sqrt(fx**2 + fy**2 + fz**2)

    # Create Butterworth filter envelope 
    filter_response = th.ones_like(freq_magnitude)
    filter_response *= 1 / (1 + (freq_magnitude/freq_cutoff)**(2 * filter_steepness))

    # Apply filter in Fourier space while preserving mean
    volume_mean = th.mean(volume)
    volume_centered = volume - volume_mean
    volume_filtered = th.fft.ifftn(th.fft.fftn(volume_centered) * filter_response)
    volume_filtered += volume_mean

    # Return real component
    return th.real(volume_filtered)

def shrink_nonneg(model, tau1, tau2):
    # xx = th.stack([th.abs(x) - tau, th.zeros_like(x)])
    # maxx, inds = th.max(xx, 0)
    # del xx
    # maxx[th.sign(x) < 0] = 0
    
    # x = model[0]
    model.real -= tau1
    model.imag -= tau2
    model.real[th.sign(model.real) < 0] = 0
    model.imag[th.sign(model.imag) < 0] = 0
    # x_complex = th.view_as_real(x)
    # x_real = x_complex[:,:,:,0]
    # x_imag = x_complex[:,:,:,1]
    # # # # x_real[th.sign(x_real) < 0] += tau1
    # # # # x_real[th.sign(x_real) > 0] -= tau1
    # # # # x_imag[th.sign(x_imag) < 0] += tau2
    # # # # x_imag[th.sign(x_imag) > 0] -= tau2
    # x_complex[:,:,:,0] -= tau1
    # x_complex[:,:,:,1] -= tau2
    # x_stack = th.stack([x_real, x_imag], 3)
    # x_new = th.view_as_complex(x_complex)
    
    # tau1_tensor = th.ones_like(model) * tau1
    # tau2_tensor = th.ones_like(model) * tau2

    # model = model - tau1_tensor - 1j*tau2_tensor
    # return x_new.unsqueeze(0)
    return model

def create_3d_gaussian_kernel(kernel_size, sigma):
    """
    Create a 3D Gaussian kernel in PyTorch. FWHM = 2.355 sigma

    Args:
        kernel_size (int): The size of the kernel (should be odd).
        sigma (float): The standard deviation of the Gaussian distribution.

    Returns:
        torch.Tensor: A 3D Gaussian kernel.
    """
    if kernel_size % 2 == 0:
        raise ValueError("Kernel size should be odd.")

    # Create a grid of coordinates
    coords = th.arange(-(kernel_size // 2), (kernel_size // 2) + 1)
    x, y, z = th.meshgrid(coords, coords, coords)

    # Calculate the Gaussian kernel
    kernel = th.exp(-(x ** 2 + y ** 2 + z ** 2) / (2 * sigma ** 2))
    kernel = kernel / kernel.sum()  # Normalize the kernel to sum to 1

    return kernel

def init_gauss_blur_regularization(kernel_size, sigma, device='cpu'):
    gauss_kernel = create_3d_gaussian_kernel(kernel_size, sigma).unsqueeze(0)
    gauss_kernel = gauss_kernel.to(device)
    gauss_kernel.requires_grad = False
    return gauss_kernel


def gaussian_blur_regularization(model, gauss_kernel):
    volume = model.detach()
    return filter3d(volume, gauss_kernel)


def gram_schmidt_orthogonalization(A):
    for i, v in enumerate(A):
        norm = np.linalg.norm(v)
        q = v / norm
        A[i] = v
        if i + 1 == len(A):
            break
        proj = np.array([np.vdot(q, a) * q for a in A[i + 1:]])
        A[i + 1:] = A[i + 1:] - proj
    return A

def orthogonalize_probe(probe, device):
    """

    :param probe: tensor: (Nmodes, 1, MY, MX) float

    :return: ortho_probe: tensor: (Nmodes, 1, MY, MX)
    inten: tensor: (Nmodes, 1)
    """
    probe2 = probe.clone().detach().cpu().numpy().squeeze(1)
    # probe2 = probex
    mod = np.sum(np.abs(probe2) ** 2, (-2, -1))
    ind = np.argsort(-mod[1:]) + 1
    probe3 = gram_schmidt_orthogonalization(probe2)
    probe2[1:, :] = probe3[ind]
    probe2 = th.as_tensor(probe2).unsqueeze(1).to(device)
    return probe2

def return_scaled_histogram_ordering(array, vmin=None, vmax=None, normalize=False):
    """
    Utility function for calculating min and max values for plotting array
    based on distribution of pixel values

    Parameters
    ----------
    array: np.array
        array to be plotted
    vmin: float
        lower fraction cut off of pixel values
    vmax: float
        upper fraction cut off of pixel values
    normalize: bool
        if True, rescales from 0 to 1

    Returns
    ----------
    scaled_array: np.array
        array clipped outside vmin and vmax
    vmin: float
        lower value to be plotted
    vmax: float
        upper value to be plotted
    """

    if vmin is None:
        vmin = 0.02
    if vmax is None:
        vmax = 0.98

    vals = np.sort(array.ravel())
    ind_vmin = np.round((vals.shape[0] - 1) * vmin).astype("int")
    ind_vmax = np.round((vals.shape[0] - 1) * vmax).astype("int")
    ind_vmin = np.max([0, ind_vmin])
    ind_vmax = np.min([len(vals) - 1, ind_vmax])
    vmin = vals[ind_vmin]
    vmax = vals[ind_vmax]

    if vmax == vmin:
        vmin = vals[0]
        vmax = vals[-1]

    scaled_array = array.copy()
    scaled_array = np.where(scaled_array < vmin, vmin, scaled_array)
    scaled_array = np.where(scaled_array > vmax, vmax, scaled_array)

    if normalize:
        scaled_array -= scaled_array.min()
        scaled_array /= scaled_array.max()
        vmin = 0
        vmax = 1

    return scaled_array, vmin, vmax

def get_bright_field_size(diffraction_pattern, min_threshold=0.01, max_threshold=0.99, num_steps=100):
    threshold_range = np.linspace(min_threshold, max_threshold, num_steps)
    radii = np.zeros(num_steps)
    
    # Calculate radius for each threshold level
    pattern_max = np.max(diffraction_pattern)
    for i in range(len(threshold_range)):
        current_threshold = threshold_range[i]
        binary_mask = diffraction_pattern > pattern_max*current_threshold
        radii[i] = np.sqrt(np.sum(binary_mask)/np.pi)
    
    # Analyze rate of change to find stable radius values
    radius_derivative = np.gradient(radii)
    valid_radius_mask = (radius_derivative <= 0) * (radius_derivative >= 2*np.median(radius_derivative))
    final_radius = np.mean(radii[valid_radius_mask])
    
    # Find center coordinates
    final_threshold = np.mean(threshold_range[valid_radius_mask])
    final_mask = diffraction_pattern > pattern_max*final_threshold
    masked_pattern = diffraction_pattern*final_mask
    height, width = np.shape(masked_pattern)
    grid_y, grid_x = np.meshgrid(np.arange(width), np.arange(height))
    total_intensity = np.sum(masked_pattern)
    center_x = np.sum(grid_x * masked_pattern) / total_intensity
    center_y = np.sum(grid_y * masked_pattern) / total_intensity
    
    return final_radius, center_x, center_y

def array_split_divpoints_ntotal(Ntotal, indices_or_sections):
    """
    Split an array into multiple sub-arrays.
    Please refer to the ``split`` documentation.  The only difference
    between these functions is that ``array_split`` allows
    `indices_or_sections` to be an integer that does *not* equally
    divide the axis. For an array of length l that should be split
    into n sections, it returns l % n sub-arrays of size l//n + 1
    and the rest of size l//n.
    See Also
    --------
    split : Split array into multiple sub-arrays of equal size.
    Examples
    --------
    >>> x = np.arange(8.0)
    >>> np.array_split(x, 3)
        [array([0.,  1.,  2.]), array([3.,  4.,  5.]), array([6.,  7.])]
    >>> x = np.arange(7.0)
    >>> np.array_split(x, 3)
        [array([0.,  1.,  2.]), array([3.,  4.]), array([5.,  6.])]
    """
    try:
        # handle array case.
        Nsections = len(indices_or_sections) + 1
        div_points = [0] + list(indices_or_sections) + [Ntotal]
    except TypeError:
        # indices_or_sections is a scalar, not an array.
        Nsections = int(indices_or_sections)
        if Nsections <= 0:
            raise ValueError('number sections must be larger than 0.')
        Neach_section, extras = divmod(Ntotal, Nsections)
        section_sizes = ([0] +
                         extras * [Neach_section + 1] +
                         (Nsections - extras) * [Neach_section])
        div_points = np.array(section_sizes, dtype=np.intp).cumsum()

    return div_points

# %% ../../nbs/util.ipynb 4
def fftshift_checkerboard(w, h):
    re = np.r_[w//2 * [-1, 1]]  # even-numbered rows
    ro = np.r_[w//2 * [1, -1]]  # odd-numbered rows
    return np.row_stack(h//2 * (re, ro))

# %% ../../nbs/util.ipynb 5
def cartesian_aberrations(qx, qy, lam, C):
    """
    Zernike polynomials in the cartesian coordinate system
    :param qx:
    :param qy:
    :param lam: wavelength in Angstrom
    :param C:   12 x D
    :return:
    """

    u = qx * lam
    v = qy * lam
    u2 = u ** 2
    u3 = u ** 3
    u4 = u ** 4
    # u5 = u ** 5

    v2 = v ** 2
    v3 = v ** 3
    v4 = v ** 4
    # v5 = v ** 5

    aberr = Param()
    aberr.C1 = C[0].unsqueeze(1).unsqueeze(1)
    aberr.C12a = C[1].unsqueeze(1).unsqueeze(1)
    aberr.C12b = C[2].unsqueeze(1).unsqueeze(1)
    aberr.C21a = C[3].unsqueeze(1).unsqueeze(1)
    aberr.C21b = C[4].unsqueeze(1).unsqueeze(1)
    aberr.C23a = C[5].unsqueeze(1).unsqueeze(1)
    aberr.C23b = C[6].unsqueeze(1).unsqueeze(1)
    aberr.C3 = C[7].unsqueeze(1).unsqueeze(1)
    aberr.C32a = C[8].unsqueeze(1).unsqueeze(1)
    aberr.C32b = C[9].unsqueeze(1).unsqueeze(1)
    aberr.C34a = C[10].unsqueeze(1).unsqueeze(1)
    aberr.C34b = C[11].unsqueeze(1).unsqueeze(1)

    chi = 0

    # r-2 = x-2 +y-2.
    chi += 1 / 2 * aberr.C1 * (u2 + v2) # r^2
    #r-2 cos(2*phi) = x"2 -y-2.
    # r-2 sin(2*phi) = 2*x*y.
    chi += 1 / 2 * (aberr.C12a * (u2 - v2) + 2 * aberr.C12b * u * v) # r^2 cos(2 phi) + r^2 sin(2 phi)
    # r-3 cos(3*phi) = x-3 -3*x*y'2. r"3 sin(3*phi) = 3*y*x-2 -y-3.
    chi += 1 / 3 * (aberr.C23a * (u3 - 3 * u * v2) + aberr.C23b * (3 * u2 * v - v3))# r^3 cos(3phi) + r^3 sin(3 phi)
    # r-3 cos(phi) = x-3 +x*y-2.
    # r-3 sin(phi) = y*x-2 +y-3.
    chi += 1 / 3 * (aberr.C21a * (u3 + u * v2) + aberr.C21b * (v3 + u2 * v))# r^3 cos(phi) + r^3 sin(phi)
    # r-4 = x-4 +2*x-2*y-2 +y-4.
    chi += 1 / 4 * aberr.C3 * (u4 + v4 + 2 * u2 * v2)# r^4
    # r-4 cos(4*phi) = x-4 -6*x-2*y-2 +y-4.
    chi += 1 / 4 * aberr.C34a * (u4 - 6 * u2 * v2 + v4)# r^4 cos(4 phi)
    # r-4 sin(4*phi) = 4*x-3*y -4*x*y-3.
    chi += 1 / 4 * aberr.C34b * (4 * u3 * v - 4 * u * v3) # r^4 sin(4 phi)
    # r-4 cos(2*phi) = x-4 -y-4.
    chi += 1 / 4 * aberr.C32a * (u4 - v4)
    # r-4 sin(2*phi) = 2*x-3*y +2*x*y-3.
    chi += 1 / 4 * aberr.C32b * (2 * u3 * v + 2 * u * v3)
    # r-5 cos(phi) = x-5 +2*x-3*y-2 +x*y-4.
    # r-5 sin(phi) = y*x"4 +2*x-2*y-3 +y-5.
    # r-5 cos(3*phi) = x-5 -2*x-3*y-2 -3*x*y-4.
    # r-5 sin(3*phi) = 3*y*x-4 +2*x-2*y-3 -y-5.
    # r-5 cos(5*phi) = x-5 -10*x-3*y-2 +5*x*y-4.
    # r-5 sin(5*phi) = 5*y*x-4 -10*x-2*y-3 +y-5.

    chi *= 2 * np.pi / lam

    return chi

# %% ../../nbs/util.ipynb 6
def memory_mb(x, dtype=None):
    if isinstance(x, th.Tensor):
        return x.nelement() * x.element_size() / 2 ** 20
    elif isinstance(x, tuple):
        assert dtype is not None, 'memory_mb: dtype must not be None'
        element_size = th.zeros(1, dtype=dtype).element_size()
        nelement = np.prod(np.asarray(x))
        return nelement * element_size / 2 ** 20


def memory_gb(x, dtype=None):
    if isinstance(x, th.Tensor):
        return x.nelement() * x.element_size() / 2 ** 30
    elif isinstance(x, tuple):
        assert dtype is not None, 'memory_mb: dtype must not be None'
        element_size = th.zeros(1, dtype=dtype).element_size()
        nelement = np.prod(np.asarray(x))
        return nelement * element_size / 2 ** 30

# %% ../../nbs/util.ipynb 7
def fourier_coordinates_2D(N, dx=[1.0, 1.0], centered=True):
    qxx = fftfreq(N[1], dx[1])
    qyy = fftfreq(N[0], dx[0])
    if centered:
        qxx += 0.5 / N[1] / dx[1]
        qyy += 0.5 / N[0] / dx[0]
    qx, qy = np.meshgrid(qxx, qyy)
    q = np.array([qy, qx]).astype(np.float32)
    return q

def get_qx_qy_1D(M, dx, dtype, fft_shifted=False):
    qxa = th.fft.fftfreq(M[0], dx[0]).to(dtype)
    qya = th.fft.fftfreq(M[1], dx[1]).to(dtype)
    if fft_shifted:
        qxa = th.fft.fftshift(qxa)
        qya = th.fft.fftshift(qya)
    return qxa, qya

# %% ../../nbs/util.ipynb 8
def array_split_divpoints(ary, indices_or_sections, axis=0):
    """
    Split an array into multiple sub-arrays.
    Please refer to the ``split`` documentation.  The only difference
    between these functions is that ``array_split`` allows
    `indices_or_sections` to be an integer that does *not* equally
    divide the axis. For an array of length l that should be split
    into n sections, it returns l % n sub-arrays of size l//n + 1
    and the rest of size l//n.
    See Also
    --------
    split : Split array into multiple sub-arrays of equal size.
    Examples
    --------
    >>> x = np.arange(8.0)
    >>> np.array_split(x, 3)
        [array([0.,  1.,  2.]), array([3.,  4.,  5.]), array([6.,  7.])]
    >>> x = np.arange(7.0)
    >>> np.array_split(x, 3)
        [array([0.,  1.,  2.]), array([3.,  4.]), array([5.,  6.])]
    """
    try:
        Ntotal = ary.shape[axis]
    except AttributeError:
        Ntotal = len(ary)
    try:
        # handle array case.
        Nsections = len(indices_or_sections) + 1
        div_points = [0] + list(indices_or_sections) + [Ntotal]
    except TypeError:
        # indices_or_sections is a scalar, not an array.
        Nsections = int(indices_or_sections)
        if Nsections <= 0:
            raise ValueError('number sections must be larger than 0.')
        Neach_section, extras = divmod(Ntotal, Nsections)
        section_sizes = ([0] +
                         extras * [Neach_section + 1] +
                         (Nsections - extras) * [Neach_section])
        div_points = np.array(section_sizes, dtype=np.intp).cumsum()

    return div_points

# %% ../../nbs/util.ipynb 9
def distance(z, x):
    """
    Distance of two complex vectors
    :param z: tensor
    :param x: tensor
    :return:
    """
    c = th.vdot(z.ravel(), x.ravel())
    phi = -th.angle(c)
    exp_minus_phi = th.exp(1j * phi)
    p = exp_minus_phi.to(x.device)
    x_hat = x * p
    res = th.norm(z - x_hat,2)
    return res

# %% ../../nbs/util.ipynb 10
def rel_dist(z, x):
    """
    Distance of two complex vectors
    :param z: tensor
    :param x: tensor
    :return:
    """
    d = distance(z, x)
    x_norm = th.norm(x,2)
    return d / x_norm

# %% ../../nbs/util.ipynb 11
PARAM_PREFIX = 'pars'
class Param(dict):
    """
    Convenience class: a dictionary that gives access to its keys
    through attributes.
    
    Note: dictionaries stored in this class are also automatically converted
    to Param objects:
    >>> p = Param()
    >>> p.x = {}
    >>> p
    Param({})
    
    While dict(p) returns a dictionary, it is not recursive, so it is better in this case
    to use p.todict(). However, p.todict does not check for infinite recursion. So please
    don't store a dictionary (or a Param) inside itself.
    
    BE: Please note also that the recursive behavior of the update function will create
    new references. This will lead inconsistency if other objects refer to dicts or Params
    in the updated Param instance. 
    """
    _display_items_as_attributes = True
    _PREFIX = PARAM_PREFIX

    def __init__(self, __d__=None, **kwargs):
        """
        A Dictionary that enables access to its keys as attributes.
        Same constructor as dict.
        """
        dict.__init__(self)
        if __d__ is not None: self.update(__d__)
        self.update(kwargs)

    def __getstate__(self):
        return self.__dict__.items()

    def __setstate__(self, items):
        for key, val in items:
            self.__dict__[key] = val

    def __repr__(self):
        return "%s(%s)" % (self.__class__.__name__, dict.__repr__(self))

    # def __str__(self):
    #     from .verbose import report
    #     return report(self,depth=7,noheader=True)

    def __setitem__(self, key, value):
        # BE: original behavior modified as implicit conversion may destroy references
        # Use update(value,Convert=True) instead
        # return super(Param, self).__setitem__(key, Param(value) if type(value) == dict else value)
        return super(Param, self).__setitem__(key, value)

    def __getitem__(self, name):
        # item = super(Param, self).__getitem__(name)
        # return Param(item) if type(item) == dict else item
        return super(Param, self).__getitem__(name)

    def __delitem__(self, name):
        return super(Param, self).__delitem__(name)

    def __delattr__(self, name):
        return super(Param, self).__delitem__(name)

    # __getattr__ = __getitem__
    def __getattr__(self, name):
        try:
            return self.__getitem__(name)
        except KeyError as ke:
            raise AttributeError(ke)

    __setattr__ = __setitem__

    def copy(self, depth=0):
        """
        :returns Param: A (recursive) copy of P with depth `depth` 
        """
        d = Param(self)
        if depth > 0:
            for k, v in d.iteritems():
                if isinstance(v, self.__class__): d[k] = v.copy(depth - 1)
        return d

    def __dir__(self):
        """
        Defined to include the keys when using dir(). Useful for
        tab completion in e.g. ipython.
        If you do not wish the dict key's be displayed as attributes
        (although they are still accessible as such) set the class 
        attribute `_display_items_as_attributes` to False. Default is
        True.
        """
        if self._display_items_as_attributes:
            return self.keys()
            # return [item.__dict__.get('name',str(key)) for key,item in self.iteritems()]
        else:
            return []

    def update(self, __d__=None, in_place_depth=0, Convert=False, **kwargs):
        """
        Update Param - almost same behavior as dict.update, except
        that all dictionaries are converted to Param if `Convert` is set 
        to True, and update may occur in-place recursively for other Param
        instances that self refers to.
        
        Parameters
        ----------
        Convert : bool 
                  If True, convert all dict-like values in self also to Param.
                  *WARNING* 
                  This mey result in misdirected references in your environment
        in_place_depth : int 
                  Counter for recursive in-place updates 
                  If the counter reaches zero, the Param to a key is
                  replaced instead of updated
        """

        def _k_v_update(k, v):
            # If an element is itself a dict, convert it to Param
            if Convert and hasattr(v, 'keys'):
                # print 'converting'
                v = Param(v)
            # new key 
            if not k in self:
                self[k] = v
            # If this key already exists and is already dict-like, update it
            elif in_place_depth > 0 and hasattr(v, 'keys') and isinstance(self[k], self.__class__):
                self[k].update(v, in_place_depth - 1)
                """
                if isinstance(self[k],self.__class__):
                    # Param gets recursive in_place updates
                    self[k].update(v, in_place_depth - 1)
                else:
                    # dicts are only updated in-place once
                    self[k].update(v)
                """
            # Otherwise just replace it
            else:
                self[k] = v

        if __d__ is not None:
            if hasattr(__d__, 'keys'):
                # Iterate through dict-like argument
                for k, v in __d__.items():
                    _k_v_update(k, v)

            else:
                # here we assume a (key,value) list.
                for (k, v) in __d__:
                    _k_v_update(k, v)

        for k, v in kwargs.items():
            _k_v_update(k, v)

        return None

    def _to_dict(self, Recursive=False):
        """
        Convert to dictionary (recursively if needed).
        """
        if not Recursive:
            return dict(self)
        else:
            d = dict(self)
            for k, v in d.items():
                if isinstance(v, self.__class__): d[k] = v._to_dict(Recursive)
        return d

    @classmethod
    def _from_dict(cls, dct):
        """
        Make Param from dict. This is similar to the __init__ call
        """
        # p=Param()
        # p.update(dct.copy())
        return Param(dct.copy())


def validate_standard_param(sp, p=None, prefix=None):
    """\
    validate_standard_param(sp) checks if sp follows the standard parameter convention.
    validate_standard_param(sp, p) attemps to check if p is a valid implementation of sp.

    NOT VERY SOPHISTICATED FOR NOW!
    """
    if p is None:
        good = True
        for k, v in sp.iteritems():
            if k.startswith('_'): continue
            if type(v) == type(sp):
                pref = k if prefix is None else '.'.join([prefix, k])
                good &= validate_standard_param(v, prefix=pref)
                continue
            else:
                try:
                    a, b, c = v
                    if prefix is not None:
                        print('    %s.%s = %s' % (prefix, k, str(v)))
                    else:
                        print('    %s = %s' % (k, str(v)))
                except:
                    good = False
                    if prefix is not None:
                        print('!!! %s.%s = %s <--- Incorrect' % (prefix, k, str(v)))
                    else:
                        print('!!! %s = %s <--- Incorrect' % (k, str(v)))

        return good
    else:
        raise RuntimeError('Checking if a param fits with a standard is not yet implemented')


def format_standard_param(p):
    """\
    Pretty-print a Standard Param class.
    """
    lines = []
    if not validate_standard_param(p):
        print('Standard parameter does not')
    for k, v in p.iteritems():
        if k.startswith('_'): continue
        if type(v) == type(p):
            sublines = format_standard_param(v)
            lines += [k + '.' + s for s in sublines]
        else:
            lines += ['%s = %s #[%s] %s' % (k, str(v[1]), v[0], v[2])]
    return lines


def asParam(obj):
    """
    Convert the input to a Param.
    
    Parameters
    ----------
    a : dict_like
        Input structure, in any format that can be converted to a Param.
        
    Returns:
    out : Param
        The Param structure built from a. No copy is done if the input
        is already a Param.  
    """
    return obj if isinstance(obj, Param) else Param(obj)


def make_default(default_dict_or_file):
    """
    convert description dict to a module dict using a possibly verbose Q & A game
    """
    pass

# %% ../../nbs/util.ipynb 12
# def single_sideband_reconstruction(G, Qx_all, Qy_all, Kx_all, Ky_all, aberrations, theta_rot, alpha_rad,
#                                    Ψ_Qp, Ψ_Qp_left_sb, Ψ_Qp_right_sb, eps, lam):
#     xp = sp.backend.get_array_module(G)
#     threadsperblock = 2 ** 8
#     blockspergrid = m.ceil(np.prod(G.shape) / threadsperblock)
#     strides = xp.array((np.array(G.strides) / (G.nbytes / G.size)).astype(np.int))
#     scale = 1
#     single_sideband_kernel[blockspergrid, threadsperblock](G, strides, Qx_all, Qy_all, Kx_all, Ky_all, aberrations,
#                                                            theta_rot, alpha_rad, Ψ_Qp, Ψ_Qp_left_sb,
#                                                            Ψ_Qp_right_sb, eps, lam, scale)
#     xp.cuda.Device(Ψ_Qp.device).synchronize()
# 
# @cuda.jit
# def single_sideband_kernel(G, strides, Qx_all, Qy_all, Kx_all, Ky_all, aberrations, theta_rot, alpha,
#                            Ψ_Qp, Ψ_Qp_left_sb, Ψ_Qp_right_sb, eps, lam, scale):
#     def aperture2(qx, qy, lam, alpha_max, scale):
#         qx2 = qx ** 2
#         qy2 = qy ** 2
#         q = m.sqrt(qx2 + qy2)
#         ktheta = m.asin(q * lam)
#         return (ktheta < alpha_max) * scale
# 
#     def chi3(qy, qx, lam, C):
#         """
#         Zernike polynomials in the cartesian coordinate system
#         :param qx:
#         :param qy:
#         :param lam: wavelength in Angstrom
#         :param C:   (12 ,)
#         :return:
#         """
# 
#         u = qx * lam
#         v = qy * lam
#         u2 = u ** 2
#         u3 = u ** 3
#         u4 = u ** 4
#         # u5 = u ** 5
# 
#         v2 = v ** 2
#         v3 = v ** 3
#         v4 = v ** 4
#         # v5 = v ** 5
# 
#         # aberr = Param()
#         # aberr.C1 = C[0]
#         # aberr.C12a = C[1]
#         # aberr.C12b = C[2]
#         # aberr.C21a = C[3]
#         # aberr.C21b = C[4]
#         # aberr.C23a = C[5]
#         # aberr.C23b = C[6]
#         # aberr.C3 = C[7]
#         # aberr.C32a = C[8]
#         # aberr.C32b = C[9]
#         # aberr.C34a = C[10]
#         # aberr.C34b = C[11]
# 
#         chi = 0
# 
#         # r-2 = x-2 +y-2.
#         chi += 1 / 2 * C[0] * (u2 + v2)  # r^2
#         # r-2 cos(2*phi) = x"2 -y-2.
#         # r-2 sin(2*phi) = 2*x*y.
#         chi += 1 / 2 * (C[1] * (u2 - v2) + 2 * C[2] * u * v)  # r^2 cos(2 phi) + r^2 sin(2 phi)
#         # r-3 cos(3*phi) = x-3 -3*x*y'2. r"3 sin(3*phi) = 3*y*x-2 -y-3.
#         chi += 1 / 3 * (C[5] * (u3 - 3 * u * v2) + C[6] * (3 * u2 * v - v3))  # r^3 cos(3phi) + r^3 sin(3 phi)
#         # r-3 cos(phi) = x-3 +x*y-2.
#         # r-3 sin(phi) = y*x-2 +y-3.
#         chi += 1 / 3 * (C[3] * (u3 + u * v2) + C[4] * (v3 + u2 * v))  # r^3 cos(phi) + r^3 sin(phi)
#         # r-4 = x-4 +2*x-2*y-2 +y-4.
#         chi += 1 / 4 * C[7] * (u4 + v4 + 2 * u2 * v2)  # r^4
#         # r-4 cos(4*phi) = x-4 -6*x-2*y-2 +y-4.
#         chi += 1 / 4 * C[10] * (u4 - 6 * u2 * v2 + v4)  # r^4 cos(4 phi)
#         # r-4 sin(4*phi) = 4*x-3*y -4*x*y-3.
#         chi += 1 / 4 * C[11] * (4 * u3 * v - 4 * u * v3)  # r^4 sin(4 phi)
#         # r-4 cos(2*phi) = x-4 -y-4.
#         chi += 1 / 4 * C[8] * (u4 - v4)
#         # r-4 sin(2*phi) = 2*x-3*y +2*x*y-3.
#         chi += 1 / 4 * C[9] * (2 * u3 * v + 2 * u * v3)
#         # r-5 cos(phi) = x-5 +2*x-3*y-2 +x*y-4.
#         # r-5 sin(phi) = y*x"4 +2*x-2*y-3 +y-5.
#         # r-5 cos(3*phi) = x-5 -2*x-3*y-2 -3*x*y-4.
#         # r-5 sin(3*phi) = 3*y*x-4 +2*x-2*y-3 -y-5.
#         # r-5 cos(5*phi) = x-5 -10*x-3*y-2 +5*x*y-4.
#         # r-5 sin(5*phi) = 5*y*x-4 -10*x-2*y-3 +y-5.
# 
#         chi *= 2 * np.pi / lam
# 
#         return chi
# 
#     gs = G.shape
#     N = gs[0] * gs[1] * gs[2] * gs[3]
#     n = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x
#     iqy = n // strides[0]
#     iqx = (n - iqy * strides[0]) // strides[1]
#     iky = (n - (iqy * strides[0] + iqx * strides[1])) // strides[2]
#     ikx = (n - (iqy * strides[0] + iqx * strides[1] + iky * strides[2])) // strides[3]
# 
#     if n < N:
# 
#         Qx = Qx_all[iqx]
#         Qy = Qy_all[iqy]
#         Kx = Kx_all[ikx]
#         Ky = Ky_all[iky]
# 
#         Qx_rot = Qx * m.cos(theta_rot) - Qy * m.sin(theta_rot)
#         Qy_rot = Qx * m.sin(theta_rot) + Qy * m.cos(theta_rot)
# 
#         Qx = Qx_rot
#         Qy = Qy_rot
# 
#         A = aperture2(Ky, Kx, lam, alpha, scale) * cm.exp(-1j * chi3(Ky, Kx, lam, aberrations))
#         chi_KplusQ = chi3(Ky + Qy, Kx + Qx, lam, aberrations)
#         A_KplusQ = aperture2(Ky + Qy, Kx + Qx, lam, alpha, scale) * cm.exp(-1j * chi_KplusQ)
#         chi_KminusQ = chi3(Ky - Qy, Kx - Qx, lam, aberrations)
#         A_KminusQ = aperture2(Ky - Qy, Kx - Qx, lam, alpha, scale) * cm.exp(-1j * chi_KminusQ)
# 
#         Γ = A.conjugate() * A_KminusQ - A * A_KplusQ.conjugate()
# 
#         Kplus = sqrt((Kx + Qx) ** 2 + (Ky + Qy) ** 2)
#         Kminus = sqrt((Kx - Qx) ** 2 + (Ky - Qy) ** 2)
#         K = sqrt(Kx ** 2 + Ky ** 2)
#         bright_field = K < alpha / lam
#         double_overlap1 = (Kplus < alpha / lam) * bright_field * (Kminus > alpha / lam)
#         double_overlap2 = (Kplus > alpha / lam) * bright_field * (Kminus < alpha / lam)
# 
#         Γ_abs = abs(Γ)
#         take = Γ_abs > eps and bright_field
#         if take:
#             val = G[iqy, iqx, iky, ikx] * Γ.conjugate()
#             cuda.atomic.add(Ψ_Qp.real, (iqy, iqx), val.real)
#             cuda.atomic.add(Ψ_Qp.imag, (iqy, iqx), val.imag)
#         if double_overlap1:
#             val = G[iqy, iqx, iky, ikx] * Γ.conjugate()
#             cuda.atomic.add(Ψ_Qp_left_sb.real, (iqy, iqx), val.real)
#             cuda.atomic.add(Ψ_Qp_left_sb.imag, (iqy, iqx), val.imag)
#         if double_overlap2:
#             val = G[iqy, iqx, iky, ikx] * Γ.conjugate()
#             cuda.atomic.add(Ψ_Qp_right_sb.real, (iqy, iqx), val.real)
#             cuda.atomic.add(Ψ_Qp_right_sb.imag, (iqy, iqx), val.imag)
#         if iqx == 0 and iqy == 0:
#             val = abs(G[iqy, iqx, iky, ikx]) + 1j * 0
#             cuda.atomic.add(Ψ_Qp.real, (iqy, iqx), val.real)
#             cuda.atomic.add(Ψ_Qp_left_sb.real, (iqy, iqx), val.real)
#             cuda.atomic.add(Ψ_Qp_right_sb.real, (iqy, iqx), val.real)

# %% ../../nbs/util.ipynb 13
import matplotlib
import matplotlib.gridspec as gridspec
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar
import matplotlib.font_manager as fm
import numpy as np
from PIL import Image

def sector_mask(shape, centre, radius, angle_range=(0,360)):
    """
    Return a boolean mask for a circular sector. The start/stop angles in
    `angle_range` should be given in clockwise order.
    """

    x, y = np.ogrid[:shape[0], :shape[1]]
    cx, cy = centre
    tmin, tmax = np.deg2rad(angle_range)

    # ensure stop angle > start angle
    if tmax < tmin:
        tmax += 2 * np.pi

    # convert cartesian --> polar coordinates
    r2 = (x - cx) * (x - cx) + (y - cy) * (y - cy)
    theta = np.arctan2(x - cx, y - cy) - tmin

    # wrap angles between 0 and 2*pi
    theta %= (2 * np.pi)

    # circular mask
    circmask = r2 <= radius * radius

    # angular mask
    anglemask = theta <= (tmax - tmin)

    return circmask * anglemask

# %% ../../nbs/util.ipynb 14
from math import sqrt
def wavelength(E_eV):
    emass = 510.99906;  # electron rest mass in keV
    hc = 12.3984244;  # h*c
    lam = hc / m.sqrt(E_eV * 1e-3 * (2 * emass + E_eV * 1e-3))  # in Angstrom
    return lam  


def DOF(alpha, E_eV):
    E0 = E_eV

    # Calculate wavelength and electron interaction parameter
    m = 9.109383 * 10 ** -31
    e = 1.602177 * 10 ** -19
    c = 299792458
    h = 6.62607 * 10 ** -34

    lam = h / sqrt(2 * m * e * E0) / sqrt(1 + e * E0 / 2 / m / c ** 2) * 10 ** 10
    DOF = 2 * lam / alpha ** 2
    return DOF

# %% ../../nbs/util.ipynb 15
# @cuda.jit
def dense_to_sparse_kernel(dense, indices, counts, frame_dimensions):
    ny, nx = cuda.grid(2)
    NY, NX, MYBIN, MXBIN = dense.shape
    MY, MX = frame_dimensions
    if ny < NY and nx < NX:
        k = 0
        for mx in range(MX):
            for my in range(MY):
                idx1d = my * MX + mx
                if dense[ny,nx,my,mx] > 0:
                    indices[ny,nx,k] = idx1d
                    counts[ny,nx,k] = dense[ny,nx,my,mx]
                    k += 1                    

def advanced_raster_scan(ny=10, nx=10, fast_axis=1, mirror=[1, 1], theta=0, dy=1, dx=1):
    """
    Generates as raster scan.
    
    Parameters
    ----------
    ny, nx : int
        Number of steps in *y* (vertical) and *x* (horizontal) direction
        *x* is the fast axis
        
    dy, dx : float
        Step size (grid spacinf) in *y* and *x*  
        2
    Returns
    -------
    pos : ndarray
        A (N,2)-array of positions.
        
    Examples
    --------
    """
    iix, iiy = np.indices((nx, ny))
    if fast_axis != 1:
        tmp = iix
        iix = iiy
        iiy = tmp

    # print iix.shape, iiy.shape
    positions = np.array([(dx * i, dy * j) for i, j in zip(iix.ravel(), iiy.ravel())]).astype(np.float32)

    mins = np.array([positions[:, 0].min(), positions[:, 1].min()])
    maxs = np.array([positions[:, 0].max(), positions[:, 1].max()])

    center = mins + (maxs - mins) / 2.0
    positions -= center

    positions[:, 0] *= mirror[0]
    positions[:, 1] *= mirror[1]

    theta_rad = theta / 180.0 * np.pi
    R = np.array([[np.cos(theta_rad), -np.sin(theta_rad)],
                  [np.sin(theta_rad), np.cos(theta_rad)]])
    # rotate counterclockwise by theta
    positions = positions.dot(R)
    mins = np.array([positions[:, 0].min(), positions[:, 1].min()])
    positions -= mins
    return positions.astype(np.float32)

# %% ../../nbs/util.ipynb 16
def advanced_raster_scan(ny=10, nx=10, fast_axis=1, mirror=[1, 1], theta=0, dy=1, dx=1):
    """
    Generates as raster scan.
    
    Parameters
    ----------
    ny, nx : int
        Number of steps in *y* (vertical) and *x* (horizontal) direction
        *x* is the fast axis
        
    dy, dx : float
        Step size (grid spacinf) in *y* and *x*  
        
    Returns
    -------
    pos : ndarray
        A (N,2)-array of positions.
        
    Examples
    --------
    """
    iiy, iix = np.indices((ny, nx))
    if fast_axis != 1:
        tmp = iix
        iix = iiy
        iiy = tmp

    # print iix.shape, iiy.shape
    positions = np.array([(dy * i, dx * j) for i, j in zip(iiy.ravel(), iix.ravel())]).astype(np.float32)

    mins = np.array([positions[:, 0].min(), positions[:, 1].min()])
    maxs = np.array([positions[:, 0].max(), positions[:, 1].max()])

    center = mins + (maxs - mins) / 2.0
    positions -= center

    positions[:, 0] *= mirror[0]
    positions[:, 1] *= mirror[1]

    theta_rad = theta / 180.0 * np.pi
    R = np.array([[np.cos(theta_rad), -np.sin(theta_rad)],
                  [np.sin(theta_rad), np.cos(theta_rad)]])
    # rotate counterclockwise by theta
    positions = positions.dot(R)
    mins = np.array([positions[:, 0].min(), positions[:, 1].min()])
    positions -= mins
    return positions.astype(np.float32)

# %% ../../nbs/util.ipynb 17
# def get_qx_qy_1D(M, dx, dtype, fft_shifted=False):
#     xp = sp.backend.get_array_module(dx)
#     qxa = xp.fft.fftfreq(M[0], dx[0]).astype(dtype)
#     qya = xp.fft.fftfreq(M[1], dx[1]).astype(dtype)
#     if fft_shifted:
#         qxa = xp.fft.fftshift(qxa)
#         qya = xp.fft.fftshift(qya)
#     return qxa, qya
# 
# 
# def get_qx_qy_2D(M, dx, dtype, fft_shifted=False):
#     xp = sp.backend.get_array_module(dx)
#     qxa = xp.fft.fftfreq(M[0], dx[0]).astype(dtype)
#     qya = xp.fft.fftfreq(M[1], dx[1]).astype(dtype)
#     [qxn, qyn] = xp.meshgrid(qxa, qya)
#     if fft_shifted:
#         qxn = xp.fft.fftshift(qxn)
#         qyn = xp.fft.fftshift(qyn)
#     return qxn, qyn

# %% ../../nbs/util.ipynb 18
import numpy as np
import torch as th
from math import sqrt

def scatter_add_patches(input: th.Tensor, out: th.Tensor, axes, positions, patch_size, reduce_dim=None) -> th.Tensor:
    """
    Scatter_adds K patches of size :patch_size: at axes [ax1, ax2] into the output tensor. The patches are added at
    positions :positions:. Additionally, several dimensions of the input tensor can be summed, specified by reduce_dims.

    :param input:   K x M1 x M2 at least 3-dimensional tensor of patches,
    :param out: at least two-dimensional tensor that is the scatter_add target
    :param axes: (2,) axes at which to scatter the input
    :param positions: K x 2 LongTensor
    :param patch_size: (2,) LongTensor
    :param reduce_dims: (N,) LongTensor
    :return: out, the target of scatter_add_
    """
    if reduce_dim is not None:
        other1 = th.split(input, 1, dim=reduce_dim)
        other = tuple()
        for one in other1:
            other += (one.squeeze_().contiguous(),)
        # now we have D tensors of shape K B M1 M2 2
    else:
        other = [input]

    r = positions
    s = patch_size
    K = r.shape[0]

    # patches has dimension  K x M1 x M2 x 2

    index0 = th.arange(s[0], device=input.device, dtype=th.long).view(s[0], 1).expand(s[0], s[1])
    index1 = th.arange(s[1], device=input.device, dtype=th.long).view(1, s[1]).expand(s[0], s[1])

    # size is patch_size
    # print(f"strides at axes: {input.stride(axes[0]), input.stride(axes[1])}")
    index = out.stride(axes[0]) * (index0 + r[:, 0].view(K, 1, 1)) + out.stride(axes[1]) * (
            index1 + r[:, 1].view(K, 1, 1))

    # print(f"new index shape: {index.shape}")
    higher_dim_offsets = th.arange(out.stride(axes[1]), device=input.device).view(1, 1, 1, out.stride(axes[1]))
    index = index.view(index.shape[0], index.shape[1], index.shape[2], 1).expand(
        (index.shape[0], index.shape[1], index.shape[2], out.stride(axes[1])))

    # print(higher_dim_offsets, higher_dim_offsets.shape, index.shape)
    index = index + higher_dim_offsets
    # now we have the K x M1 x M2 x 2 indices into the N1 x N2 x 2 array

    # print(f"new index shape: {index.shape}")
    # index = index.view(index.shape[0], 1, index.shape[1], index.shape[2], index.shape[3]).expand(
    #     (index.shape[0], B, index.shape[1], index.shape[2], index.shape[3]))

    # print(f"max index   : {th.max(index.view(-1))}")
    # print(f"len   out   : {out.view(-1).shape[0]}")
    # print(f"index shape : {index.view(-1).shape[0]}")
    # print(f"others shape: {other[0].view(-1).shape[0]}")

    for i, one in enumerate(other):
        # print(f"others [{i}] shape: {one.view(-1).shape[0]}")
        # print(one.shape)
        # print(index.shape)
        out.view(-1).scatter_add_(0, index.view(-1), one.view(-1))
    return out

# %% ../../nbs/util.ipynb 19
def gather_patches(input, axes, positions, patch_size, out=None) -> th.Tensor:
    """
    Gathers K patches of size :patch_size: at axes [ax1, ax2] of the input tensor. The patches are collected started at
    K positions pos.

    if :input: is an n-dimensional tensor with size (x_0, x_1, x_2, ..., x_a, x_ax1, x_ax2, x_b, ..., x_{n-1})
    then :out: is an n-dimensional tensor with size  (K, x_0, x_1, x_2, ..., x_a, patch_size[0], patch_size[1], x_3, ..., x_{n-1})

    :param input: at least two-dimensional tensor
    :param axes: axes at which to gather the patches
    :param positions: K x 2 LongTensor
    :param patch_size: (2,) LongTensor
    :param out: n-dimensional tensor with size  (K, x_0, x_1, x_2, ..., x_a, patch_size[0], patch_size[1], x_3, ..., x_{n-1})
    :return:
    """
    # print(f"input shape: {input.shape}")
    # print(f"positions.dtype {positions.dtype}")
    r = positions
    s = patch_size
    K = positions.shape[0]

    # condense all dimensions x_0 ... x_a
    dim0size = th.prod(th.Tensor([input.shape[:axes[0]]])).int().item() if axes[0] > 0 else 1
    view = [dim0size]
    for d in input.shape[axes[0]:]:
        view.append(d)
    y = input.view(th.Size(view)).squeeze()

    index0 = th.arange(s[0], device=input.device, dtype=th.long).view(s[0], 1).expand(s[0], s[1])
    index1 = th.arange(s[1], device=input.device, dtype=th.long).view(1, s[1]).expand(s[0], s[1])

    # size is patch_size
    # print(f"strides at axes: {input.stride(axes[0]), input.stride(axes[1])}")
    # print(index0.dtype, r.dtype)
    index = input.stride(axes[0]) * (index0 + r[:, 0].view(K, 1, 1)) + input.stride(axes[1]) * (
            index1 + r[:, 1].view(K, 1, 1))
    # print(f"new index shape: {index.shape}")
    higher_dim_offsets = th.arange(input.stride(axes[1]), device=input.device).view(1, 1, 1, input.stride(axes[1]))
    index = index.view(index.shape[0], index.shape[1], index.shape[2], 1).expand(
        (index.shape[0], index.shape[1], index.shape[2], input.stride(axes[1])))
    # print(higher_dim_offsets, higher_dim_offsets.shape, index.shape)
    index = index + higher_dim_offsets
    # print(f"new index shape: {index.shape}")
    index = index.view(index.shape[0], 1, index.shape[1], index.shape[2], index.shape[3]).expand(
        (index.shape[0], dim0size, index.shape[1], index.shape[2], index.shape[3]))
    # print(f"new index shape: {index.shape}")
    lower_dim_offset = th.arange(dim0size, device=input.device) * y.stride(0)
    lower_dim_offset = lower_dim_offset.view(1, dim0size, 1, 1, 1).long()
    index = index + lower_dim_offset
    # print(f"new index shape: {index.shape}")
    index = index.contiguous().view(-1)
    out = th.index_select(y.view(-1), 0, index, out=out)

    out_view = (K,)
    for ax in input.shape[:axes[0]]:
        out_view += (ax,)
    out_view += (patch_size[0].item(),)
    out_view += (patch_size[1].item(),)
    for ax in input.shape[axes[1] + 1:]:
        out_view += (ax,)

    out = out.view(out_view)
    return out



# %% ../../nbs/util.ipynb 20
def array_split_divpoints(ary, indices_or_sections, axis=0):
    """
    Split an array into multiple sub-arrays.
    Please refer to the ``split`` documentation.  The only difference
    between these functions is that ``array_split`` allows
    `indices_or_sections` to be an integer that does *not* equally
    divide the axis. For an array of length l that should be split
    into n sections, it returns l % n sub-arrays of size l//n + 1
    and the rest of size l//n.
    See Also
    --------
    split : Split array into multiple sub-arrays of equal size.
    Examples
    --------
    >>> x = np.arange(8.0)
    >>> np.array_split(x, 3)
        [array([0.,  1.,  2.]), array([3.,  4.,  5.]), array([6.,  7.])]
    >>> x = np.arange(7.0)
    >>> np.array_split(x, 3)
        [array([0.,  1.,  2.]), array([3.,  4.]), array([5.,  6.])]
    """
    try:
        Ntotal = ary.shape[axis]
    except AttributeError:
        Ntotal = len(ary)
    try:
        # handle array case.
        Nsections = len(indices_or_sections) + 1
        div_points = [0] + list(indices_or_sections) + [Ntotal]
    except TypeError:
        # indices_or_sections is a scalar, not an array.
        Nsections = int(indices_or_sections)
        if Nsections <= 0:
            raise ValueError('number sections must be larger than 0.')
        Neach_section, extras = divmod(Ntotal, Nsections)
        section_sizes = ([0] +
                         extras * [Neach_section + 1] +
                         (Nsections - extras) * [Neach_section])
        div_points = np.array(section_sizes, dtype=np.intp).cumsum()

    return div_points

# %% ../../nbs/util.ipynb 21
import numpy as np
import matplotlib
import matplotlib.gridspec as gridspec
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar
import matplotlib.font_manager as fm
import numpy as np
from PIL import Image
from ase import units
import torch as th
import matplotlib.gridspec as gridspec
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar
import matplotlib.font_manager as fm
import torch.nn as nn
from numpy.fft import fftfreq, fftshift
from skimage.filters import gaussian

PARAM_PREFIX = 'pars'
class Param(dict):
    """
    Convenience class: a dictionary that gives access to its keys
    through attributes.

    Note: dictionaries stored in this class are also automatically converted
    to Param objects:
    >>> p = Param()
    >>> p.x = {}
    >>> p
    Param({})

    While dict(p) returns a dictionary, it is not recursive, so it is better in this case
    to use p.todict(). However, p.todict does not check for infinite recursion. So please
    don't store a dictionary (or a Param) inside itself.

    BE: Please note also that the recursive behavior of the update function will create
    new references. This will lead inconsistency if other objects refer to dicts or Params
    in the updated Param instance.
    """
    _display_items_as_attributes = True
    _PREFIX = PARAM_PREFIX

    def __init__(self, __d__=None, **kwargs):
        """
        A Dictionary that enables access to its keys as attributes.
        Same constructor as dict.
        """
        dict.__init__(self)
        if __d__ is not None: self.update(__d__)
        self.update(kwargs)

    def __getstate__(self):
        return self.__dict__.items()

    def __setstate__(self, items):
        for key, val in items:
            self.__dict__[key] = val

    def __repr__(self):
        return "%s(%s)" % (self.__class__.__name__, dict.__repr__(self))

    # def __str__(self):
    #     from .verbose import report
    #     return report(self,depth=7,noheader=True)

    def __setitem__(self, key, value):
        # BE: original behavior modified as implicit conversion may destroy references
        # Use update(value,Convert=True) instead
        # return super(Param, self).__setitem__(key, Param(value) if type(value) == dict else value)
        return super(Param, self).__setitem__(key, value)

    def __getitem__(self, name):
        # item = super(Param, self).__getitem__(name)
        # return Param(item) if type(item) == dict else item
        return super(Param, self).__getitem__(name)

    def __delitem__(self, name):
        return super(Param, self).__delitem__(name)

    def __delattr__(self, name):
        return super(Param, self).__delitem__(name)

    # __getattr__ = __getitem__
    def __getattr__(self, name):
        try:
            return self.__getitem__(name)
        except KeyError as ke:
            raise AttributeError(ke)

    __setattr__ = __setitem__

    def copy(self, depth=0):
        """
        :returns Param: A (recursive) copy of P with depth `depth`
        """
        d = Param(self)
        if depth > 0:
            for k, v in d.iteritems():
                if isinstance(v, self.__class__): d[k] = v.copy(depth - 1)
        return d

    def __dir__(self):
        """
        Defined to include the keys when using dir(). Useful for
        tab completion in e.g. ipython.
        If you do not wish the dict key's be displayed as attributes
        (although they are still accessible as such) set the class
        attribute `_display_items_as_attributes` to False. Default is
        True.
        """
        if self._display_items_as_attributes:
            return self.keys()
            # return [item.__dict__.get('name',str(key)) for key,item in self.iteritems()]
        else:
            return []

    def update(self, __d__=None, in_place_depth=0, Convert=False, **kwargs):
        """
        Update Param - almost same behavior as dict.update, except
        that all dictionaries are converted to Param if `Convert` is set
        to True, and update may occur in-place recursively for other Param
        instances that self refers to.

        Parameters
        ----------
        Convert : bool
                  If True, convert all dict-like values in self also to Param.
                  *WARNING*
                  This mey result in misdirected references in your environment
        in_place_depth : int
                  Counter for recursive in-place updates
                  If the counter reaches zero, the Param to a key is
                  replaced instead of updated
        """

        def _k_v_update(k, v):
            # If an element is itself a dict, convert it to Param
            if Convert and hasattr(v, 'keys'):
                # print 'converting'
                v = Param(v)
            # new key
            if not k in self:
                self[k] = v
            # If this key already exists and is already dict-like, update it
            elif in_place_depth > 0 and hasattr(v, 'keys') and isinstance(self[k], self.__class__):
                self[k].update(v, in_place_depth - 1)
                """
                if isinstance(self[k],self.__class__):
                    # Param gets recursive in_place updates
                    self[k].update(v, in_place_depth - 1)
                else:
                    # dicts are only updated in-place once
                    self[k].update(v)
                """
            # Otherwise just replace it
            else:
                self[k] = v

        if __d__ is not None:
            if hasattr(__d__, 'keys'):
                # Iterate through dict-like argument
                for k, v in __d__.items():
                    _k_v_update(k, v)

            else:
                # here we assume a (key,value) list.
                for (k, v) in __d__:
                    _k_v_update(k, v)

        for k, v in kwargs.items():
            _k_v_update(k, v)

        return None

    def _to_dict(self, Recursive=False):
        """
        Convert to dictionary (recursively if needed).
        """
        if not Recursive:
            return dict(self)
        else:
            d = dict(self)
            for k, v in d.items():
                if isinstance(v, self.__class__): d[k] = v._to_dict(Recursive)
        return d

    @classmethod
    def _from_dict(cls, dct):
        """
        Make Param from dict. This is similar to the __init__ call
        """
        # p=Param()
        # p.update(dct.copy())
        return Param(dct.copy())
# Cell
def cartesian_aberrations(qx, qy, lam, C):
    """
    Zernike polynomials in the cartesian coordinate system
    :param qx:
    :param qy:
    :param lam: wavelength in Angstrom
    :param C:   12 x D
    :return:
    """

    u = qx * lam
    v = qy * lam
    u2 = u ** 2
    u3 = u ** 3
    u4 = u ** 4
    # u5 = u ** 5

    v2 = v ** 2
    v3 = v ** 3
    v4 = v ** 4
    # v5 = v ** 5

    aberr = Param()
    aberr.C1 = C[0].unsqueeze(1).unsqueeze(1)
    aberr.C12a = C[1].unsqueeze(1).unsqueeze(1)
    aberr.C12b = C[2].unsqueeze(1).unsqueeze(1)
    aberr.C21a = C[3].unsqueeze(1).unsqueeze(1)
    aberr.C21b = C[4].unsqueeze(1).unsqueeze(1)
    aberr.C23a = C[5].unsqueeze(1).unsqueeze(1)
    aberr.C23b = C[6].unsqueeze(1).unsqueeze(1)
    aberr.C3 = C[7].unsqueeze(1).unsqueeze(1)
    aberr.C32a = C[8].unsqueeze(1).unsqueeze(1)
    aberr.C32b = C[9].unsqueeze(1).unsqueeze(1)
    aberr.C34a = C[10].unsqueeze(1).unsqueeze(1)
    aberr.C34b = C[11].unsqueeze(1).unsqueeze(1)

    chi = 0

    # r-2 = x-2 +y-2.
    chi += 1 / 2 * aberr.C1 * (u2 + v2) # r^2
    #r-2 cos(2*phi) = x"2 -y-2.
    # r-2 sin(2*phi) = 2*x*y.
    chi += 1 / 2 * (aberr.C12a * (u2 - v2) + 2 * aberr.C12b * u * v) # r^2 cos(2 phi) + r^2 sin(2 phi)
    # r-3 cos(3*phi) = x-3 -3*x*y'2. r"3 sin(3*phi) = 3*y*x-2 -y-3.
    chi += 1 / 3 * (aberr.C23a * (u3 - 3 * u * v2) + aberr.C23b * (3 * u2 * v - v3))# r^3 cos(3phi) + r^3 sin(3 phi)
    # r-3 cos(phi) = x-3 +x*y-2.
    # r-3 sin(phi) = y*x-2 +y-3.
    chi += 1 / 3 * (aberr.C21a * (u3 + u * v2) + aberr.C21b * (v3 + u2 * v))# r^3 cos(phi) + r^3 sin(phi)
    # r-4 = x-4 +2*x-2*y-2 +y-4.
    chi += 1 / 4 * aberr.C3 * (u4 + v4 + 2 * u2 * v2)# r^4
    # r-4 cos(4*phi) = x-4 -6*x-2*y-2 +y-4.
    chi += 1 / 4 * aberr.C34a * (u4 - 6 * u2 * v2 + v4)# r^4 cos(4 phi)
    # r-4 sin(4*phi) = 4*x-3*y -4*x*y-3.
    chi += 1 / 4 * aberr.C34b * (4 * u3 * v - 4 * u * v3) # r^4 sin(4 phi)
    # r-4 cos(2*phi) = x-4 -y-4.
    chi += 1 / 4 * aberr.C32a * (u4 - v4)
    # r-4 sin(2*phi) = 2*x-3*y +2*x*y-3.
    chi += 1 / 4 * aberr.C32b * (2 * u3 * v + 2 * u * v3)
    # r-5 cos(phi) = x-5 +2*x-3*y-2 +x*y-4.
    # r-5 sin(phi) = y*x"4 +2*x-2*y-3 +y-5.
    # r-5 cos(3*phi) = x-5 -2*x-3*y-2 -3*x*y-4.
    # r-5 sin(3*phi) = 3*y*x-4 +2*x-2*y-3 -y-5.
    # r-5 cos(5*phi) = x-5 -10*x-3*y-2 +5*x*y-4.
    # r-5 sin(5*phi) = 5*y*x-4 -10*x-2*y-3 +y-5.

    chi *= 2 * np.pi / lam

    return chi

class ZernikeProbe2(nn.Module):
    def __init__(self, q: th.Tensor, lam, fft_shifted=True):
        """
        Creates an aberration surface from aberration coefficients. The output is backpropable

        :param q: 2 x M1 x M2 tensor of x coefficients of reciprocal space
        :param lam: wavelength in Angstrom
        :param C: aberration coefficients
        :return: (maximum size of all aberration tensors) x MY x MX
        """

        super(ZernikeProbe2, self).__init__()
        self.q = q
        self.lam = lam
        self.fft_shifted = fft_shifted

        if self.fft_shifted:
            cb = fftshift_checkerboard(self.q.shape[1] // 2, self.q.shape[2] // 2)
            self.cb = th.from_numpy(cb).float().to(q.device)

    def forward(self, C, A):
        chi = cartesian_aberrations(self.q[1], self.q[0], self.lam, C)
        Psi = th.exp(-1j*chi) * A.expand_as(chi)

        if self.fft_shifted:
            Psi = Psi * self.cb

        return Psi

def mosaic(data):
    n, w, h = data.shape
    diff = np.sqrt(n) - int(np.sqrt(n))
    s = np.sqrt(n)
    m = int(s)
    if diff > 1e-6: m += 1
    mosaic = np.zeros((m * w, m * h)).astype(data.dtype)
    for i in range(m):
        for j in range(m):
            if (i * m + j) < n:
                mosaic[i * w:(i + 1) * w, j * h:(j + 1) * h] = data[i * m + j]
    return mosaic

# Cell
def plotmosaic(img, title='Image', savePath=None, cmap='hot', show=True, figsize=(10, 10), vmax=None):
    fig, ax = plt.subplots(figsize=figsize)
    mos = mosaic(img)
    cax = ax.imshow(mos, interpolation='nearest', cmap=plt.cm.get_cmap(cmap), vmax=vmax)
    cbar = fig.colorbar(cax)
    ax.set_title(title)
    plt.grid(False)
    plt.show()
    if savePath is not None:
        fig.savefig(savePath + '.png', dpi=600)

def aperture(q: th.Tensor, lam, alpha_max, edge=2):
    ktheta = th.asin(q.norm(dim=0) * lam)
    qmax = alpha_max / lam
    dk = q[0][1][0]

    arr = th.zeros_like(q[1])
    arr[ktheta < alpha_max] = 1
    if edge > 0:
        dEdge = edge / (qmax / dk)  # fraction of aperture radius that will be smoothed
        # some fancy indexing: pull out array elements that are within
        #    our smoothing edges
        ind = (ktheta / alpha_max > (1 - dEdge)) * (ktheta / alpha_max < (1 + dEdge))
        arr[ind] = 0.5 * (1 - th.sin(np.pi / (2 * dEdge) * (ktheta[ind] / alpha_max - 1)))
    return arr

def get_qx_qy_2D(M, dx, dtype, fft_shifted=False):
    qxa = np.fft.fftfreq(M[0], dx[0]).astype(dtype)
    qya = np.fft.fftfreq(M[1], dx[1]).astype(dtype)
    [qxn, qyn] = np.meshgrid(qxa, qya)
    if fft_shifted:
        qxn = np.fft.fftshift(qxn)
        qyn = np.fft.fftshift(qyn)
    return qxn, qyn

def zplot(imgs, suptitle='Image', savePath=None, cmap=['hot', 'hsv'], title=['Abs', 'Phase'], show=True,
          figsize=(9, 5), scale=None):
    im1, im2 = imgs
    fig = plt.figure(figsize=figsize, dpi=300)
    fig.suptitle(suptitle, fontsize=15, y=0.9)
    gs1 = gridspec.GridSpec(1, 2)
    gs1.update(wspace=0, hspace=0)  # set the spacing between axes.
    ax1 = plt.subplot(gs1[0])
    ax2 = plt.subplot(gs1[1])
    div1 = make_axes_locatable(ax1)
    div2 = make_axes_locatable(ax2)

    imax1 = ax1.imshow(im1, interpolation='nearest', cmap=plt.cm.get_cmap(cmap[0]))
    imax2 = ax2.imshow(im2, interpolation='nearest', cmap=plt.cm.get_cmap(cmap[1]))

    cax1 = div1.append_axes("left", size="10%", pad=0.4)
    cax2 = div2.append_axes("right", size="10%", pad=0.4)

    cbar1 = plt.colorbar(imax1, cax=cax1)
    cbar2 = plt.colorbar(imax2, cax=cax2)

    cax1.yaxis.set_ticks_position('left')
    ax2.yaxis.set_ticks_position('right')

    ax1.set_title(title[0])
    ax2.set_title(title[1])

    if scale is not None:
        fontprops = fm.FontProperties(size=18)
        scalebar = AnchoredSizeBar(ax1.transData,
                                   scale[0], scale[1], 'lower right',
                                   pad=0.1,
                                   color='white',
                                   frameon=False,
                                   size_vertical=im1.shape[0] / 40,
                                   fontproperties=fontprops)

        ax1.add_artist(scalebar)

    ax1.grid(False)
    ax2.grid(False)

    if show:
        plt.show()
    if savePath is not None:
        # print 'saving'
        fig.savefig(savePath + '.png')


def plotAbsAngle(img, suptitle='Image', savePath=None, cmap=['gray', 'gray'], title=['Abs', 'Phase'], show=True,
                 figsize=(10, 10), scale=None):
    zplot([np.abs(img), np.angle(img)], suptitle, savePath, cmap, title, show, figsize, scale)

def relativistic_mass_correction(energy: float) -> float:
    return (1 + units._e * energy / (units._me * units._c ** 2))

def energy_to_mass(energy: float) -> float:
    """
    Calculate relativistic mass from energy.

    Parameters
    ----------
    energy: float
        Energy [eV].

    Returns
    -------
    float
        Relativistic mass [kg]̄
    """

    return relativistic_mass_correction(energy) * units._me

def energy_to_wavelength(energy: float) -> float:
    return units._hplanck * units._c / np.sqrt(
        energy * (2 * units._me * units._c ** 2 / units._e + energy)) / units._e * 1.e10

def energy_to_sigma(energy: float) -> float:

    return (2 * np.pi * energy_to_mass(energy) * units.kg * units._e * units.C * energy_to_wavelength(energy) / (
            units._hplanck * units.s * units.J) ** 2)

def make_probe(df, E, alpha_rad, dx_angstrom, D, M1):
    lam = energy_to_wavelength(E)
    q = th.as_tensor(get_qx_qy_2D([M1, M1], [dx_angstrom, dx_angstrom], np.float32, fft_shifted=False))
    Ap = th.as_tensor(aperture(q, lam, alpha_rad)) * 1e2
    Ap = fftshift(gaussian(fftshift(Ap.cpu().numpy()), 0.7))
    Ap = th.as_tensor(Ap)

    Psi_gen = ZernikeProbe2(q, lam, fft_shifted=True)

    C_target = th.zeros((12, D))

    for i in range(D):
        C_target[0, i] = df + i * 100

    probe_target0 = Psi_gen(C_target, Ap)
    probe_target0 = th.fft.ifft2(probe_target0, norm='ortho')
    probe_target0 += th.randn_like(probe_target0) * 1e-3
    probe_target0 = probe_target0.unsqueeze(0)
    return probe_target0

def make_probe2(df, E, dx_angstrom, D, M1, Ap):
    lam = energy_to_wavelength(E)
    q = th.as_tensor(get_qx_qy_2D([M1, M1], [dx_angstrom, dx_angstrom], np.float32, fft_shifted=False))

    Psi_gen = ZernikeProbe2(q, lam, fft_shifted=True)

    C_target = th.zeros((12, D))

    for i in range(D):
        C_target[0, i] = df + i * 100

    probe_target0 = Psi_gen(C_target, Ap)
    probe_target0 = th.fft.ifft2(probe_target0, norm='ortho')
    probe_target0 += th.randn_like(probe_target0) * 1e-3
    probe_target0 = probe_target0.unsqueeze(1)
    return probe_target0
