# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/operators.loss.ipynb.

# %% auto 0
__all__ = ['amplitude_loss_per_pattern', 'AmplitudeLoss', 'amplitude_loss']

# %% ../../nbs/operators.loss.ipynb 2
from nbdev.showdoc import *
import torch as th
from ..kernels import amplitude_loss_function
from ..core import dtype_complex, dtype_real

# %% ../../nbs/operators.loss.ipynb 3
class AmplitudeLoss(th.autograd.Function):

    @staticmethod
    def forward(ctx, a_model, a_target):
        loss = th.zeros((a_model.shape[0],), device=a_model.device, dtype=dtype_real)
        # grad = th.zeros_like(a_model, device=a_model.device, dtype=dtype_real)
        amplitude_loss_function(a_model.data, a_target.data, loss.data)
        loss.requires_grad = True
        ctx.save_for_backward(a_target)
        return loss

    @staticmethod
    def backward(ctx, *grad_outputs):
        grad_a_model, = ctx.saved_tensors
        grad_a_target = None
        # print(f'AmplitudeLoss.backward any NaN: {th.any(th.isnan(grad_a_model))}')
        return grad_a_model, grad_a_target

amplitude_loss_per_pattern = AmplitudeLoss.apply

def amplitude_loss(a_model, a_target):
    loss_per_pattern = amplitude_loss_per_pattern(a_model, a_target)
    loss = th.sum(loss_per_pattern)
    return loss, loss_per_pattern
